---
title: "A Complete Guide to GPT — About AI, Humanity, and Future"
excerpt: "Apr 09, 2023, Tech review"
date: 2023-04-09
categories:
  - Blog
---

Recent news about GPT has been flying around, with social medias and short videos stirring up a frenzy. It seems quite necessary to write a tech review to discuss this matter. This GPT craze is about to bring significant change, and the first step to be prepared is to understand the technology itself. Get ready folks, as we are about to dive deep.

This tech review is quite lengthy, and readers are welcomed to read sections of interest based on the subheadings. If you don’t usually follow cutting-edge AI technology but are interested or concerned about AI’s progress, please spend about fifteen minutes with me, as we will go through the most discussed topics one by one.

This review starts with technical details and addresses several rumors frequently mentioned on social medias. It gradually explores the impact of GPT and the AI industry on society and the future. At the same time, the article places great emphasis on readability, making it simpler than essays!

*[This review was originally written in Chinese and translated by GPT-4, and edited by me]*

# Table of Content
## Q & A about GPT
- What is GPT?
- How does GPT work? **[Important]**
- Does GPT understand our questions and its response?
- Does GPT has consciousness?
- Does GPT think and reason?
- Does GPT represent the approach of the Singularity?
- What jobs will GPT replace?
- Is GPT an Artificial General Intelligence?
- Will GPT-5 or GPT-6 surpass human?

## Thoughts about GPT **[Also Important]**
- The Great Debate of the Century: Pausing the development of GPT for six months
- AI or IA? Enhancing Human Intelligence!
- About AI, humanity, and future
- Easter eggs

---

## What is GPT?
GPT stands for Generative Pretrained Transformer, which is a large-scale language model based on deep learning and falls under the category of artificial intelligence (AI). GPT is trained on vast amounts of text data from the Internet and has numerous parameters. It can provide high-quality textual responses based on textual input. Its capabilities include but are not limited to: text translation, article refinement, generating common text templates, answering questions, do your homework, making convincing nonsense, and accomplishing any language-related tasks. Since programming code is also a form of language, GPT has reasonably strong coding abilities and can independently complete relatively simple programming tasks.

Compared to previous AIs, GPT has made noticeable improvements in text generation and answering questions. Due to its outstanding language abilities, GPT is considered the first step for modern AI towards achieving Artificial General Intelligence (AGI). (AGI refers to the ability to perform multiple independent tasks, rather than just excelling in a single domain. We will get back to this later.)

---

## How does GPT work?
*[It is recommended to read this section, as multiple sections following this will reference this content.]*

At its core, **GPT is a statistics-based probability model**. It predicts the probability of the next word appearing based on the input question and previous answers, then selects the word with the highest probability for output. This process repeats until GPT believes it has generated a sufficient response.

How can GPT consistently produce reasonable sentences? This brings us to the question of how GPT is trained. GPT’s training process relies on vast amounts of data, with the ultimate goal of **finding correlations between datasets**. To understand this, consider the following examples: if I say “salt,” you might think of “pepper.” If I say “2+2,” you might think of “ 4.” These pairs of phrases have a strong statistical correlation: if the first one appears, the second one will probably appear as well. If we collect all the text from the entire Internet and use it to train GPT, it will learn the correlations between the language and the text we use, and can **mimic** our writing style and linguistic habits.

To reiterate, this entire process is based on statistical probability predictions, selecting and generating the next word with the highest probability by reading previous text. Also, GPT’s capabilities are limited to reading and outputting text only (though the latest generation of GPT is said to accept images and other forms of input, this does not affect its underlying mechanism however).

---

## Does GPT understand our questions and its response?
**Depends on your definition of understanding.**

There is a famous thought experiment called the “Chinese Room” problem in philosophy: In a room, there is a person who doesn’t understand Chinese but has an all-purpose Chinese translation dictionary (or, for a more modern example, a translation app). Outside the room, a person who speak Chinese keeps send in notes with Chinese sentences on. The person inside can consult the dictionary, follow some guidance, and write perfect responses in Chinese and send them back out. In this process, the person inside is simply following the rules in the dictionary to write appropriate Chinese words. To the people outside however, it seems that the person inside the room understands Chinese. The “Chinese Room” problem points out the contradictory of different definitions of “understanding.” Do you think the person inside understands Chinese? Is it the person who understands Chinese, or the person with the dictionary, or the person and the room as a whole, or no one at all?

GPT is exactly the same with this analogy, with the person replaced by GPT and the dictionary replaced by a probability prediction model. If you understand this problem, you would probably agree that GPT is just a Chinese Room and does not possess human-level language understanding.

*A side: There are some unconventional responses to this problem. One of my AI course professor takes a direct approach, believing that humans are also Chinese Rooms, and our understanding of natural language is based on some statistical model operating in the brain. Thus, he argues that there is no fundamental difference between GPT and us. My viewpoint is similar, but I prefer the idea that both our brain’s underlying mechanisms and GPT’s statistical basis are Chinese Rooms, while the real “understanding” occurs at some higher level. GPT has only a very limited capability of understand, and its understanding mechanism is fundamentally different from ours. I am currently trying to argue for this viewpoint, and I welcome discussions with anyone with a background in philosophy or cognitive science. Plz help, or else I won’t be able to finish my essay!*

---

## Does GPT have consciousness?
Here comes one of the most controversial questions, and we will answer more later on.

**No, and it will never have.**

There are some differences between consciousness and self-consciousness. Since different theories in Cognitive Science can lead to different conclusions, we will focus on self-consciousness here.

The claim that GPT has consciousness originated from an AI researcher at Google. The researcher claimed that an AI (not even GPT at the time) passed a series of social, moral, and self-awareness tests, suggesting that such AI has self-awareness. I have no idea under what cultural background and mental state the researcher came to this conclusion, it seems more like a publicity stunt to gain more attention and research funding. As mentioned earlier, GPT’s training objective is to find correlations between data, and the data it learns from is entirely human-generated text from the Internet. It is reasonable to believe that a normal human being can pass the social, moral, and self-awareness tests. Therefore, by learning from human-generated text and dialogue, GPT will naturally learn this correlation and mimic human language patterns. This explains why GPT can pass these so-called tests.

By the way, GPT can pass the Turing Test. Not too surprising If you’ve read this far. The Turing Test itself is limited by its time and has many flaws. Assessing AI’s intelligence requires a more comprehensive test, which is also limited by our understanding of human intelligence and progress of cognitive science.

We have just shown that the researcher’s claim is wrong, but is it really impossible for GPT to have self-consciousness? The internal structure of GPT is called a “neural network,” which may sound related to the brain but really is not. The fundamental of neural networks is linear algebra and nonlinear activation functions, and its computations are all about probability predictions. Do you believe that self-awareness, the ability to recognize itself as an entity, can emerge from such structure? I think any neural network-based AI will never be able to develop self-consciousness.

But wait, isn’t it said that GPT-4 has 100 billion parameters, almost catching up with the number of human brain cells? Unfortunately, the neurons in GPT are merely mathematical concepts: weighted sums and activation functions, which have nothing to do with biological neurons at all. Simply comparing their numbers is meaningless. The ocean contains countless water molecules and various organic substances, making it quite complex. Does the ocean have consciousness then? Yes? Great, you must have read *Solaris*!

---

## Does GPT think and reason?
**It doesn’t think and has no logic.**

When we talk about thinking, we generally refer to performing a series of actions such as reasoning, judgment, and decision-making. As previously mentioned, however, GPT is just a **probability prediction model based on statistics**. All of GPT’s operations can be summarized as predicting the next most likely word based on previous conversation text. You might think this is wrong since GPT seems to hold structured conversations and can even solve mathematical proof! **This is in fact a misconception**. GPT’s training data consists of human-generated text, and majority of the text we produce is clear and logical. Therefore, by learning from logical texts, GPT can master common sentence structure and even derivation patterns. After finding this correlation, GPT can consistently output clear and seemingly logical answers, but this by no means implies that GPT understands what it’s saying.

A typical example is GPT’s response in specialized fields. Since there is less text data in specialized fields compared to everyday conversation, GPT is unable to learn due to a lack of data. When GPT knows common derivation sentence patterns but does not understand the knowledge itself, it will confidently spout nonsense. GPT’s answers can be so convincing that it can trick laypeople, but experts will find many errors and logical flaws.

So, what about mathematical skill? Mathematical formulas are a type of formal language, and a large amount of related text can be easily obtained from the Internet for GPT’s training data. Therefore, GPT can perform relatively simple and straightforward mathematical reasoning. However, GPT is extremely poor at mathematical calculations, so much so that it later had to be connected to an external calculator. The reason is simple: you might find text online that states 2+2=4, but you may not find the result of 114514 * 147258. **Without data, GPT immediately becomes confused. GPT’s learning process is all about the statistical correlation between words and sentences, not the logic and reasoning behind the text**. As a result, GPT would think that pi is larger than 3.2 because pi is an infinite decimal and it’s longer. Makes absolutely no sense.

At this point, we have to bring up a logical ability test I created: Give GPT a 22-bit binary number and ask it to convert it to hexadecimal, while explaining its thought process and reasoning. Both GPT-3 and GPT-4 failed this test, making mistakes such as copying wrong numbers, omitting numbers, and packaging binaries from the left instead of the right. Surprisingly, although the problem-solving process is chaotic, the thought process is entirely correct! (Why is that? This is left as an exercise to the reader.) I named this test the “Future Test” and welcome everyone to try it out on any new language models in the future.

---

## Does GPT represent the approach of the Singularity?
Let’s first explain what the Singularity is. It is a concept originating from physics, where a singularity is a point with mass but no volume, resulting in infinite density and the breakdown of many physical laws. The concept of “technological singularity” is generally used today, popularized mainly by Ray Kurzweil.

People use this term to vividly express their expectations and concerns about the pace of AI development: AI has been advancing faster and faster, from simple robots in the 1950s to AlphaGo, which defeated a human Go champion in 2016, and now GPT just scares and shocks everyone. If AI’s development continues to accelerate, there will come a day when AI’s intelligence surpasses that of humans and continues to grow exponentially, leading to an infinitely increasing level of intelligence, which is the Singularity in AI development.

Based on this definition, it is not difficult to see that GPT does not meet the prerequisites. One major assumption of the Singularity is that AI’s capabilities will continue to grow at an accelerating pace. However, in reality, GPT’s performance is constrained by various factors, such as the number of parameters and the size of training data. Further improvement of GPT’s capabilities requires continuous investment by human. To meet the prerequisites of the Singularity, AI needs to be able to learn autonomously and self-iterate. In this regard, it seems that AlphaGo, which continually plays against itself, might be more likely to break through the Singularity. However, AlphaGo’s learning is merely about its parameters, and its underlying architecture will always restrict its upper limit of capabilities. It does not have the ability to iterate and optimize its own architecture.

In conclusion, to reach the Singularity, AI needs to be able to continuously iterate and optimize itself and overcome all constraints that affect its level of intelligence. Unfortunately (or fortunately?), we do not know how to build such an AI.

---

## What jobs will be replaced by GPT?
The jobs most impacted by GPT are those that don’t involve complex thinking but involve extensive interaction with text, as no one is better at playing on words (literally) than GPT. However, GPT will not completely replace any profession, as it can only output text, while **lacking autonomy and the ability to independently complete tasks**. GPT also lacks logical capabilities, so it **cannot engage in complex planning and reasoning**. Therefore, GPT always need human guidance: people with expertise in specific fields set goals and plans, break down overall goals into simple objectives, and have GPT (or other types of AI) assist in achieving them to improve work efficiency and productivity.

**Common Viewpoint 1: GPT will replaces delivery workers**
(Delivery is a HUGE market in China, and everyone is worrying about AI takes their jobs.)

This viewpoint mainly argues that delivery workers have no technical barriers and do not involve complex mental labor, making them easy to replace. However, **delivering requires a highly matured general intelligence**, as delivery workers need to constantly analyze and make judgments about their ever-changing surroundings and plan reasonable routes, making decisions in unexpected situations.

Sounds familiar? This is just autonomous driving, but even harder. GPT, as a language model, doesn’t possess any of these capabilities, and the decades-old autonomous driving industry is far from reaching the level required for delivery. In fact, since aerial environments are much less complex than ground traffic, it is more likely that delivery workers will be replaced by drones with limited autonomous navigation capabilities, which require professional human pilots for remote control, responsible for complex tasks such as takeoff, landing, and delivery.

**Common Viewpoint 2: GPT will replaces programmers**
The main basis for this viewpoint is that large language models like GPT have demonstrated strong programming capabilities, leading to the belief that programmers will soon be replaced. It’s true that GPT can write code, but it can only complete tasks that can be found in online tutorials. Since GPT only learns the relationships between texts, the absence of tutorials means no training data, and no training data means no learning.

Although most code can be found ready-made online, the core challenge of programming lies in understanding the interactions and logical connections between different modules, which is a complexity beyond GPT’s capabilities. In addition to coding, programmers also need to understand software and hardware environments, know how to compile and deploy products, and maintain code in the long term. GPT, which can only output text, is clearly unable to complete any of these tasks independently.

(Replace programmers? Do my assignment first!)

It cannot be denied that GPT (and Github Copilot) are very powerful programming assistants. Such AI can easily assist with small code segments and provide ideas for programmers at any time, but this still relies on the premise of human expert planning and guidance.

**Common Viewpoint 3: GPT replaces artists**
Artists (drawing specifically) are indeed one of the most threatened professions today, but this is not due to GPT. A few months ago, image generating AI has become quite mature. There are now a considerable number of AI specialized in generating images, such as DALL-E, Stable Diffusion, and Midjourney.

Why are artists so affected? Because these AI can already consistently produce artwork that meets some user needs. The whole process is very simple, it only takes users ten to thirty minutes to learn the basic techniques of prompting. From coming up with ideas to image output, **users can complete the entire task independently without involving professional artists**.

In fact, previous image generation AI has already caused quite a stir in the artist community, with one of the main issues being that beginners can use AI to generate artworks and profit after just a few hours of learning , while the data used to train the AI is taken without permission from the original creators. This issue also exists in the case of GPT, as its training data comes from online articles and publications, but not many people complain about GPT infringing on copyright.

My interpretation is that this is a panic response to AI’s ability to create on a level comparable to human capabilities, In other fields, it manifests as “AI taking our jobs,” while in the artist community, it appears as “AI infringement.” Another explanation is that many online texts have entered the public domain in terms of copyright, but the data used for AI-generated artwork mostly consists of popular, recently-created images. Readers with other explanations are more than welcomed to discuss with me.

**So, what jobs will be replaced by GPT in the end?**
In the long run, what kind of professions will be replaced? There is an important conclusion that we will visit again later: AI still relies on the guidance and planning of human experts, and those who can use AI will significantly boost the productivity. Therefore, no industry will be completely replaced, but those who know how to use AI to assist in their work will undoubtedly replace those who don’t.

---

## Is GPT an Artificial General Intelligence?
Artificial General Intelligence (AGI), also known as general AI, refers to AI that can perform multiple tasks, not just excel in a single domain, and typically requires the ability to learn across different fields. The definition of AGI is somewhat vague. In cognitive science, AGI is considered a machine that **fully replicates human brain functionality**, including emotions, self-awareness, etc. In the field of computer science, people tend to emphasize on the problem solving ability: AGI represents the AI that can autonomously learn and solve problems in different domains, which is more similar to a **general problem solver**.

Firstly, according to the definition in cognitive science, GPT is not an AGI, not even close. As mentioned earlier, GPT is just a “Chinese room,” lacking self-awareness and emotions, and its operations are all about probability calculation, which is fundamentally different from how human mind work.

However, according to the computer science standard, GPT is indeed the first step towards AGI. GPT’s language processing ability is on par with humans. Although it is a weak AI specialized in a specific domain (language processing), GPT performs well in all subtasks within this broad category. In terms of language capabilities alone, GPT has basically reached the level of AGI.

So, what is the next step to become an AGI in computer science standard? To excel in other fields, such as logical reasoning, learning to interact with environment, learning to respond to other types of input, and many many more. Unfortunately, GPT is just a language model and, in principle, does not possess the potential to achieve any of these capabilities.

**Will GPT-5 or GPT-6 surpass human?**
In terms of text manipulation, GPT has already surpassed the average human level. However, **GPT is essentially a weak AI that excels in specific tasks but does not possess general intelligence and learning abilities applicable to other domains**. Let’s lower our expectations a bit and aim to surpass squirrels first. Can GPT do that? No. Although squirrels are just squirrels, they possess powerful general intelligence, including acting autonomously, setting goals and planning, and adaptation to the environment. GPT does not understand the environment; its entire world consists only of the input text. Even if we put GPT on a robot body, it would not be able to perform any activities beyond outputting text.

Due to architectural limitations, GPT can use text to explain how to walk, explain basic physical knowledge, and explain the common pursuit of benefits and avoidance of harm in animal intelligence, but **GPT does not know the logic behind these texts and how to interact with the real-world environment**. Recall the Future test; GPT can prefectly explain the process and reasoning of converting binary to hexadecimal , but it just can’t get the calculations right!

So, if the AI industry continues to develop, is it possible for AI to surpass humans? To surpass humans, AI must first be a **strong AI**. Unfortunately, we do not yet have a viable theory for this. One of the problems that need to be solved to achieve strong artificial intelligence is autonomous planning and logical abilities. Due to the underlying architectural constraints, modern AI based on machine learning and neural networks does not possess these capabilities.

Yann LeCun (we’ll get back to him later) recently proposed the concept of **autonomous AI and the world models** [https://openreview.net/pdf?id=BZ5a1r-kVsf], which can interact and learn from the world. This could be another path to a true AGI beyong machine learning, but it requires further exploration and study. As for the logical abilities of AI, researchers in the 1960s had already attempted this. Experience shows that AI systems based on symbolic reasoning and logic are not suitable for most problems that require “common sense.” Consequently, researchers shifted their focus from logic to neural networks. However, it seems that lack of logical ability in modern AI still need to be achieved through other methods, such as revisiting symbolic reasoning (which is also Yann LeCun’s view).

*Or we can just wait for the cognitive science people to figure out how human mind work. Then, we replicate brain functionality in machines and add whatever augmentation we want.*

---

## The Great Debate of the Century: Pausing the development of GPT for six months
In recent days, a “Treaty on the Non-Proliferation of AI” has gone viral on social media: an open letter calling for OpenAI (the company behind GPT) to suspend the development of GPT-4 and the more advanced models has gained the support of numerous entrepreneurs and scientists. Over a thousand people have signed the letter, including industry leaders like Elon Musk. Let’s take a look at the key figures and their viewpoints on both sides of the debate.

**In favor of suspending GPT development:**
Elon Musk, a major figure in the industry, co-founder of OpenAI, has never stopped attracting public’s attention, you all know him.

Yoshua Bengio, one of the three “Godfathers of AI,” received the Turing Award in 2018.

The main argument of the supporters is that GPT’s capabilities have introduced significant uncertainties, and our understanding and control of the technology itself lag behind its capabilities. Before GPT impacts more jobs and even social stability, they argue that we should pause cutting-edge research and focus on understanding and constraining the technology, as well as preparing human society for more challenges.

**Opposed to suspending development:**
Yann Lecun, another “Godfather of AI,” and Turing Award co-winner with Yoshua Bengio, argues that GPT-4 actually finished developing six months before its announcement. He claims that “suspending research” is essentially “secretly research,” and a forced suspension would only widen the gap between OpenAI and other companies and research institutions, causing more problems and hindering the overall development of the industry.

Andrew Ng, a renowned scholar and advocate for AI-related industries and education, was also my first teacher in AI (public online courses count!). Ng’s main viewpoint is that we should not fear the capabilities of GPT. Instead, we should see it as a **general-purpose technology** (also GPT) and actively explore its potential at the application level, integrating it into other industries. If led by human experts from other fields, GPT can significantly improve work efficiency, provide solutions for many industries, and ultimately increase productivity.

Additionally, most of my AI course professors also oppose suspending development. Their main arguments are similar to Yann Lecun’s, emphasizing that the uncontrollable factors brought about by a forced suspension may be even more significant than those from continuing development. Also, no government agency in the world can pause similar research on a global scale.

Interestingly, the last “Godfather of AI,” Geoffrey Hinton, who is also a prominent figure at our university, has not made any public statements on the matter.

**My opinion:**
Personally, I belong to the techno-optimist camp, advocating that **all problems brought about by technology should be solved with equivalent technological solutions**. The only issue is that our understanding of AI is indeed very limited, and we cannot restrict a black box that operates entirely on probabilities at the foundational level. In fact, **science and technology have always been and will be controllable and predictable before the emergence of truly autonomous strong AI. The only factor that can introduce uncertainty is humans**. At this stage, limiting the dangerous use of AI may require addressing the problem from social and human perspectives, and we indeed need to spend more time pondering this issue.

I also strongly agree with Andrew Ng’s viewpoint that GPT, as a general-purpose technology, has tremendous potential. Now is the perfect time to develop its potential and apply it to other industries. Throughout history, the vast majority of problems faced by human civilization can ultimately be traced back to issues of productivity. Improving productivity is the only way to address most problems at their source.

---

## AI or IA? Enhancing Human Intelligence!
A concept has garnered some attention in recent years, that instead of focusing solely on AI, maybe we should consider Intelligence Augmentation (IA): **The purpose of developing AI is to assist and empower humans, not to replace them**. Contrary to fully autonomous AI, IA emphasizes integrating AI into our work to increase productivity and efficiency.

I personally thing this is a brilliant idea. Ordinary people, AI researchers, or engineers engaged in integration of AI should always keep this in mind. Industries integrated with AI should continue to let human experts lead decision-making, coordinate work, and formulate plans, with AI completing other tasks. There are three good reasons for it:

Firstly, AI will not have the ability to independently complete complex processes in the short term.

Secondly, this approach offers a safer way to gradually integrate AI into our daily lives without causing social disruptions or massive unemployment.

Lastly, AI should not replace human creativity but should complement our weaknesses and amplify our strengths. As technology continues to develop, the gap between AI and humans will gradually narrow. If critical work relies on AI with humans as assistants, big problems will arise. What problems? Let’s leave that to the last section.

---

## About AI, Humanity, and the Future
AI has become a force that can change the rules of industries, and the transformations it brings are no less significant than any previous industrial revolution (I said in junior high that the next industrial revolution would be AI!). We have caught the wave of AI, and while the new technology is full of risks and uncertainties, it also brings numerous opportunities. AI will reshape our world, and we should actively adapt to it and integrate it into various industries, developing the potential of new technologies and promoting the growth of productivity.

New technologies will inevitably replace some jobs, but new ecosystems and industry models will create many new ones. One example is that the quality of answers from large language models like GPT is heavily dependent on the user-input prompts. Although I believe this is a temporary shortcoming in the development of technology, some industries have already seen demand for prompt engineers. Mastering the use of prompts has become a skill that can improve productivity.

There is another thought-provoking question: do we really need more powerful AI? I would like to thank Prof. Vervaeke, as each discussion with him always gives me new insights. When AI possesses general intelligence comparable to humans, we must consider whether to treat AI as independent entities with basic rights and citizenships similar to ours. When AI has the ability to set goals and plan actions autonomously, we must consider the motives behind their goals and whether they align with our interests.

It can be foreseen that, in the distant future, AI with true general intelligence will separate from us and become an independent species. They can self-replicate, have independent motives, plan and execute actions autonomously, and most importantly, **their fundamental interests may not align with ours**. The planets has limited space and resources, and we haven’t even managed the resources allocation among ethical groups well. How will we reconcile the conflicts between humans and machines in the future? This is why practicing IA is essential. When the time comes, IA will grant us more flexibility to face uncertainties and changes. We are more likely to take the initiative and resolve the issue before it becomes too late.

Maybe our current weak AI is sufficient? Maybe we can forcibly write in commands, like Asimov’s Three Laws of Robotics? Or maybe we can achieve mind uploading in the future, and everyone turns into metal cans, eliminating major differences? Future problems require future solutions. For now, let’s focus on what we can do: take GPT seriously, don’t panic, maintain optimism, and keep learning!

---

Thank you for reading this article! I was originally planning to debunk some myths about GPT from the perspective of a CS student, but I ended up writing much more than I expected.

However, this article merely serves as an introduction to all related topics. I didn’t get too deep into each topics and only covers some basic ideas. If you are interested in any of these subjects, there are lots of great books and article available that explore them in greater depth.

Again, don’t panic. Get to know the technology itself, so we can be prepared for the future.

---

## Easter Egg: Why not use Asimov’s Three Laws of Robotics to limit AI?
The Three Laws were proposed by the renowned science fiction writer and science popularizer, Isaac Asimov, even before the research on first-generation symbolic AI began. The Three Laws cannot be implemented on logic-based AI systems because the real world presents countless scenarios, requiring an infinite number of rules to cover all situations (thanks again to Prof. Vervaeke). The Three Laws also cannot be applied to neural network-based AI systems because their underlying mechanisms is probability calculations rather than logical deductions. It might be possible to simulate these laws at a statistical level (training another AI to judge), but there would be a certain “probability of failure,” or the problem of how to regulate the regulator.